{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf704dc0a9f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import data\n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__C = edict()\n",
    "args = __C\n",
    "\n",
    "### data and file related arguments\n",
    "args.dataset = 'diag'\n",
    "args.data_path = './scripts/diag_dim60'\n",
    "args.emb_path = '../diag_embedding.txt'\n",
    "args.save_path = '../results'\n",
    "args.batch_size = 1000\n",
    "\n",
    "### model-related arguments\n",
    "\n",
    "args.num_topics = 10\n",
    "args.rho_size = 60\n",
    "args.emb_size = 60\n",
    "args.t_hidden_size = 800\n",
    "args.theta_act = 'relu'\n",
    "args.train_embeddings = 1\n",
    "\n",
    "### optimization-related arguments\n",
    "\n",
    "args.lr = 0.005\n",
    "args.lr_factor = 4.0\n",
    "args.epochs = 100\n",
    "args.mode = 'eval'  #train or eval\n",
    "args.optimizer = 'adam'\n",
    "args.seed = 2019\n",
    "args.enc_drop = 0.0\n",
    "args.clip = 0.0\n",
    "args.nonmono = 10\n",
    "args.wdecay = 1.2e-6\n",
    "args.anneal_lr = 0\n",
    "args.bow_norm = 1\n",
    "\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "\n",
    "args.num_words = 10  # number of words for topic viz\n",
    "args.log_interval = 40\n",
    "args.visualize_every = 10\n",
    "args.eval_batch_size = 1000\n",
    "# args.load_from = './results/etm_diag_K_40_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_0'\n",
    "# args.load_from = ''\n",
    "args.tc = 1\n",
    "args.td = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec17_etm_diag_K_25_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_35_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_45_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_20_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_5_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_15_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_30_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n",
      "Dec17_etm_diag_K_40_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_60_trainEmbeddings_1\n"
     ]
    }
   ],
   "source": [
    "for fn in os.listdir():\n",
    "    if fn.startswith('Dec'):\n",
    "        print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-44700af87f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('\\n')\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "vocab, train, valid, test = data.get_data(os.path.join(args.data_path))\n",
    "vocab_size = len(vocab)\n",
    "args.vocab_size = vocab_size\n",
    "\n",
    "# 1. training data\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "args.num_docs_train = len(train_tokens)\n",
    "\n",
    "# 2. dev set\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "args.num_docs_valid = len(valid_tokens)\n",
    "\n",
    "# 3. test data\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "args.num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "args.num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "args.num_docs_test_2 = len(test_2_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = args.load_from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/ETM/results\n"
     ]
    }
   ],
   "source": [
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_valid)), args.eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(args.num_docs_test_1)), args.eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, args.vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if args.bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, args.vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## get document completion perplexities\n",
    "    test_ppl = evaluate(model, 'test', tc=args.tc, td=args.td)\n",
    "\n",
    "    ## get most used topics\n",
    "    indices = torch.tensor(range(args.num_docs_train))\n",
    "    indices = torch.split(indices, args.batch_size)\n",
    "    thetaAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    thetaWeightedAvg = torch.zeros(1, args.num_topics).to(device)\n",
    "    cnt = 0\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, args.vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        cnt += sums.sum(0).squeeze().cpu().numpy()\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        theta, _ = model.get_theta(normalized_data_batch)\n",
    "        thetaAvg += theta.sum(0).unsqueeze(0) / args.num_docs_train\n",
    "        weighed_theta = sums * theta\n",
    "        thetaWeightedAvg += weighed_theta.sum(0).unsqueeze(0)\n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            print('batch: {}/{}'.format(idx, len(indices)))\n",
    "    thetaWeightedAvg = thetaWeightedAvg.squeeze().cpu().numpy() / cnt\n",
    "    print('\\nThe 10 most used topics are {}'.format(thetaWeightedAvg.argsort()[::-1][:10]))\n",
    "\n",
    "    ## show topics\n",
    "    beta = model.get_beta()\n",
    "    topic_indices = list(np.random.choice(args.num_topics, 10)) # 10 random topics\n",
    "    print('\\n')\n",
    "    for k in range(args.num_topics):#topic_indices:\n",
    "        gamma = beta[k]\n",
    "        top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "        print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "    if args.train_embeddings:\n",
    "        ## show etm embeddings \n",
    "        try:\n",
    "            rho_etm = model.rho.weight.cpu()\n",
    "        except:\n",
    "            rho_etm = model.rho.cpu()\n",
    "#             queries = ['andrew', 'woman', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "#                             'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "        queries = ['5855']\n",
    "        print('\\n')\n",
    "        print('ETM embeddings...')\n",
    "        for word in queries:\n",
    "            print('word: {} .. etm neighbors: {}'.format(word, nearest_neighbors(word, rho_etm, vocab)))\n",
    "        print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
